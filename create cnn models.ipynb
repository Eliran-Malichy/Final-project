{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b62838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if it's your first run please install the following packages\n",
    "#!pip install opencv-python\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorflow\n",
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "\n",
    "## to use the gpu with tensorflow please install\n",
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# to run on the cpu unmark the next line\n",
    "## os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2536a4",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(data_Directory, classes):\n",
    "    datas_final=[]\n",
    "    labels_final=[]\n",
    "    datas_valid=[]\n",
    "    labels_valid=[]\n",
    "    for category in classes:\n",
    "        datas=[]\n",
    "        labels=[]\n",
    "        path = os.path.join(data_Directory, category)\n",
    "        class_num = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                image = cv2.imread(os.path.join(path, img))\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                new_array = cv2.resize(gray, (img_size, img_size))\n",
    "                datas.append(new_array)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(datas, labels, test_size=0.33, random_state=42)\n",
    "        datas_final.extend(x_train)\n",
    "        labels_final.extend(y_train)\n",
    "        datas_valid.extend(x_valid)\n",
    "        labels_valid.extend(y_valid)    \n",
    "    return (datas_final, labels_final, datas_valid, labels_valid)\n",
    "\n",
    "def create_testing_data(data_Directory, classes):\n",
    "    datas_final=[]\n",
    "    labels_final=[]\n",
    "    for category in classes:\n",
    "        datas=[]\n",
    "        labels=[]\n",
    "        path = os.path.join(data_Directory, category)\n",
    "        class_num = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                image = cv2.imread(os.path.join(path, img))\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                new_array = cv2.resize(gray, (img_size, img_size))\n",
    "                datas.append(new_array)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        datas_final.extend(datas)\n",
    "        labels_final.extend(labels)\n",
    "    return (datas_final, labels_final)\n",
    "\n",
    "def random_list(datas,labels):\n",
    "    temp = list(zip(datas, labels))\n",
    "    random.shuffle(temp)\n",
    "    datas_temp, lables_temp = zip(*temp)\n",
    "    return(datas_temp, lables_temp)\n",
    "\n",
    "def create_model(num_classes,input_shape=(48,48,1)):\n",
    "    # first input model\n",
    "    visible = Input(shape=input_shape, name='input')\n",
    "\n",
    "    #the 1-st block\n",
    "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
    "    conv1_2 = BatchNormalization()(conv1_2)\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
    "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n",
    "\n",
    "    #the 2-nd block\n",
    "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
    "    conv2_2 = BatchNormalization()(conv2_2)\n",
    "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
    "    conv2_2 = BatchNormalization()(conv2_3)\n",
    "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
    "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)\n",
    "\n",
    "    #the 3-rd block\n",
    "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
    "    conv3_2 = BatchNormalization()(conv3_2)\n",
    "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
    "    conv3_3 = BatchNormalization()(conv3_3)\n",
    "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
    "    conv3_4 = BatchNormalization()(conv3_4)\n",
    "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
    "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)\n",
    "\n",
    "    #the 4-th block\n",
    "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
    "    conv4_1 = BatchNormalization()(conv4_1)\n",
    "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
    "    conv4_2 = BatchNormalization()(conv4_2)\n",
    "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
    "    conv4_3 = BatchNormalization()(conv4_3)\n",
    "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
    "    conv4_4 = BatchNormalization()(conv4_4)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
    "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
    "\n",
    "    #the 5-th block\n",
    "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
    "    conv5_1 = BatchNormalization()(conv5_1)\n",
    "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
    "    conv5_2 = BatchNormalization()(conv5_2)\n",
    "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
    "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)\n",
    "\n",
    "    #Flatten and output\n",
    "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
    "    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)\n",
    "\n",
    "    # create model \n",
    "    model = Model(inputs =visible, outputs = ouput)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def print_prediction_accuracy(model,data_test,labels_test):\n",
    "    pred2=model.predict(data_test)\n",
    "    PRED=[]\n",
    "    for item in pred2:\n",
    "        value2=np.argmax(item)      \n",
    "        PRED+=[value2]\n",
    "    print(classification_report(labels_tests, PRED,labels=labels_names, target_names=target_names))    \n",
    "    cm = confusion_matrix(labels_tests, PRED,labels=labels_names)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=target_names)\n",
    "    disp = disp.plot(cmap=plt.cm.Blues,values_format='g')\n",
    "    plt.show()\n",
    "    accuracy=accuracy_score(labels_test,PRED)\n",
    "    print(\"accuracy of: \",accuracy)\n",
    "\n",
    "def print_Training_vs_validation_accuracy_graph(history):\n",
    "    get_acc = history.history['accuracy']\n",
    "    value_acc = history.history['val_accuracy']\n",
    "    epochs = range(len(get_acc))\n",
    "    plt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\n",
    "    plt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\n",
    "    plt.title('Training vs validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    plt.figure()\n",
    "    plt.show()   \n",
    "\n",
    "def print_Training_vs_validation_loss_graph(history):\n",
    "    get_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "    epochs = range(len(get_loss))\n",
    "    plt.plot(epochs, get_loss, 'r', label='Loss of Training data')\n",
    "    plt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\n",
    "    plt.title('Training vs validation loss')\n",
    "    plt.legend(loc=0)\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389b397",
   "metadata": {},
   "source": [
    "## main run\n",
    "## config variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57cfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Directory_Positive = \"archive/train/positive/\"  # positive emotions training dataset\n",
    "data_Directory_Negative = \"archive/train/negative/\"  # negative emotions training dataset\n",
    "data_Directory_All = \"archive_all/train/\" # all emotions training dataset\n",
    "data_test_Directory_Positive = \"archive/test/positive/\"  # positive emotions testing dataset\n",
    "data_test_Directory_Negative = \"archive/test/negative/\"  # negative emotions testing dataset\n",
    "data_test_Directory_All = \"archive_all/test/\" # all emotions testing dataset\n",
    "classes_Positive = [\"happy\", \"neutral\", \"surprise\"]  # list of positive emotions\n",
    "classes_Negative = [\"angry\", \"fear\", \"sad\"]  # list of negative emotions\n",
    "classes_all = [\"angry\", \"fear\",\"happy\", \"neutral\", \"sad\",\"surprise\"] # list of all emotions\n",
    "batch_size = 64\n",
    "img_size = 48\n",
    "num_epochs = 100\n",
    "model_type = 1  ##  0 - create negative model, 1- create positive model, 2 - create all emotions model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad2fc6",
   "metadata": {},
   "source": [
    "## preparations of variables according to model type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea633d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create negative traning data\n",
    "if model_type==0:\n",
    "    print(\"creates negative dataset and training set\")\n",
    "    data_Directory=data_Directory_Negative\n",
    "    data_test_Directory=data_test_Directory_Negative\n",
    "    classes=classes_Negative\n",
    "    num_classes = 3\n",
    "    target_names = classes_Negative\n",
    "    labels_names = [0,1,2]\n",
    "    model_name= 'modelNegative.h5'\n",
    "    best_model_name= 'best_negative_model.h5'\n",
    "    log_name= 'training_negative.log'\n",
    "\n",
    "if model_type==1:## create postive traning data\n",
    "    print(\"creates positive dataset and training set\")\n",
    "    data_Directory=data_Directory_Positive\n",
    "    data_test_Directory=data_test_Directory_Positive\n",
    "    classes=classes_Positive\n",
    "    num_classes = 3\n",
    "    target_names = classes_Positive\n",
    "    labels_names = [0,1,2] \n",
    "    model_name= 'modelPositive.h5'\n",
    "    best_model_name= 'best_positive_model.h5'\n",
    "    log_name= 'training_positive.log'    \n",
    "\n",
    "if model_type==2:## create all emotions traning data\n",
    "    print(\"creates all dataset and training set\")\n",
    "    data_Directory=data_Directory_All\n",
    "    data_test_Directory=data_test_Directory_All\n",
    "    classes=classes_all\n",
    "    num_classes = 6\n",
    "    target_names = classes_all\n",
    "    labels_names = [0,1,2,3,4,5]\n",
    "    model_name= 'modelAll.h5'\n",
    "    best_model_name= 'best_All_model.h5'\n",
    "    log_name= 'training_all.log'    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c7068",
   "metadata": {},
   "source": [
    "## create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9652eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas,labels,datas_valid,labels_valid = create_training_data(data_Directory, classes)\n",
    "random_list(datas,labels)\n",
    "random_list(datas_valid,labels_valid)\n",
    "X_train = np.array(datas, np.float32).reshape(-1,img_size, img_size, 1)\n",
    "X_val = np.array(datas_valid, np.float32).reshape(-1,img_size, img_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb4eee",
   "metadata": {},
   "source": [
    "## create testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_tests,labels_tests =create_testing_data(data_test_Directory, classes)\n",
    "random_list(datas_tests,labels_tests)\n",
    "X_test = np.array(datas_tests, np.float32).reshape(-1,img_size, img_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4351211",
   "metadata": {},
   "source": [
    "## convert labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c30a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(labels, num_classes) \n",
    "y_val = np_utils.to_categorical(labels_valid, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad1b3c",
   "metadata": {},
   "source": [
    "## config image data genrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24debddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator( \n",
    "    rescale=1./255,\n",
    "    rotation_range = 10,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode = 'nearest')\n",
    "\n",
    "val_gen = ImageDataGenerator( \n",
    "    rescale=1./255\n",
    "    )\n",
    "data_gen.fit(X_train)\n",
    "train_flow = data_gen.flow(X_train, y_train, batch_size=batch_size) \n",
    "val_flow = val_gen.flow(X_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede4d8f",
   "metadata": {},
   "source": [
    "## create callbacks and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = best_model_name\n",
    "csv_logger = CSVLogger(log_name)\n",
    "log_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=chk_path,\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min',\n",
    "                             moniter='val_loss')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]\n",
    "adam = optimizers.Adam(learning_rate=0.0001, decay=1e-6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9c881",
   "metadata": {},
   "source": [
    "## create model and run cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f250122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(num_classes)\n",
    "model.compile(loss= \"categorical_crossentropy\", optimizer = adam, metrics = [\"accuracy\"])\n",
    "history = model.fit(train_flow, \n",
    "                steps_per_epoch=len(X_train) / batch_size, \n",
    "                epochs=num_epochs,  \n",
    "                verbose=1,  \n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=val_flow,  \n",
    "                validation_steps=len(X_val) / batch_size)  \n",
    "model.save(model_name) #saving the model to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6828e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished createing model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9c52c",
   "metadata": {},
   "source": [
    "## display accuracy, confusion matrix and graphs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction_accuracy(model,X_test/255.,labels_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e84c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Training_vs_validation_accuracy_graph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Training_vs_validation_loss_graph(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
